import { f as createComponent, k as renderComponent, r as renderTemplate, u as unescapeHTML } from './astro/server_D72iM745.mjs';
import 'kleur/colors';
import { $ as $$BlogPost } from './BlogPost_0gCcc43l.mjs';

const html = () => "<p>In a <a href=\"https://agilegadgets.com/blog/react-hugging-face-inference-api\">prior post</a>, we learned how to write a React app that uses a Hugging Face ML model to predict data. But what if there’s no ML model that fits your needs? Well, if you have a bunch of data, you can train your own model. Over the next few minutes, we’ll show you how to find a dataset on Kaggle, train a model in Google Colab, and save it for future use.</p>\n<p>Let’s say that you want to be able to predict the star rating of a product based on the comment that the user is leaving. If they say “Best product ever!”, they’ll probably give it 5 stars. If they say “This product sucks!”, they’ll probably give it 1 star.</p>\n<p>We will want to:</p>\n<ol>\n<li><a href=\"#step-1-find-a-dataset-on-kaggle\">Find a dataset on Kaggle.</a></li>\n<li><a href=\"#step-2-set-up-google-colab\">Create a Google Colab notebook to train our model.</a></li>\n<li><a href=\"#step-3-get-the-kaggle-dataset-into-google-colab\">Get the dataset into Google Colab.</a></li>\n<li><a href=\"#step-4-clean-the-data\">Wrangle the data to get it ready for training.</a></li>\n<li><a href=\"#step-6-split-the-data-into-training-and-testing-sets\">Split the data into training and testing sets.</a></li>\n<li><a href=\"#step-7-vectorize-the-text-data\">Convert the text data into a numerical format.</a></li>\n<li><a href=\"#step-8-train-a-machine-learning-model\">Train a model to predict the star rating based on the review.</a></li>\n<li><a href=\"#step-9-evaluate-the-model\">Evaluate the model to see how well it works.</a></li>\n<li><a href=\"#step-10-perform-a-sanity-check\">Perform a sanity check to see if the model works.</a></li>\n<li><a href=\"#step-11-save-the-model-for-future-use\">Save the trained model so we can use it in an app.</a></li>\n</ol>\n<h2 id=\"step-1-find-a-dataset-on-kaggle\">Step 1: Find a Dataset on Kaggle</h2>\n<p><a href=\"https://www.kaggle.com/\">Kaggle</a> has a ton of datasets and they’re organized well.</p>\n<ol>\n<li>Go to <a href=\"https://www.kaggle.com/datasets\">Kaggle Datasets</a>.\nYou can either browse the datasets or use the search box to find a dataset that fits your needs. I’m going to search for “product reviews”. The first result I see is called <a href=\"https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Product Reviews</a>.</li>\n<li>Once you’ve found a dataset, download it to your local machine and unzip it.</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>Tip:</strong> <a href=\"https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab\">There are ways</a> to get the dataset into Colab without downloading it to your local machine, but for this tutorial, we’ll keep it simple and download it locally.</p>\n</blockquote>\n<hr>\n<h2 id=\"step-2-set-up-google-colab\">Step 2: Set Up Google Colab</h2>\n<p>Google Colab is a free cloud-based Jupyter notebook environment.</p>\n<ol>\n<li>Go to <a href=\"https://colab.research.google.com/\">Google Colab</a>.</li>\n<li>Create a <strong>+New Notebook</strong>.</li>\n<li>You might want to rename the notebook to something meaningful. I’m going with “Kaggle Product Reviews”.</li>\n<li>Upload your Kaggle dataset to the notebook by clicking on the folder icon in the left sidebar and hitting the “Upload to session storage” button.</li>\n</ol>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> The uploaded file will only be available during the current session. If you close the notebook, you’ll need to re-upload the file.</p>\n</blockquote>\n<hr>\n<h2 id=\"step-3-get-the-kaggle-dataset-into-google-colab\">Step 3: Get the Kaggle dataset into Google Colab</h2>\n<ol>\n<li>Import the file.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pd.read_csv(</span><span style=\"color:#9ECBFF\">'Reviews.csv'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  print</span><span style=\"color:#E1E4E8\">(df.head())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Error: Reviews.csv not found. Make sure the file exists.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#E1E4E8\"> pd.errors.ParserError:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Error: Couldn't parse Reviews.csv. Please check the file format.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"An unexpected error occurred: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<ol start=\"2\">\n<li>Understand the data.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df.head()  </span><span style=\"color:#6A737D\"># Display the first few rows of the dataset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.shape  </span><span style=\"color:#6A737D\"># Get the shape of the dataset (rows, columns)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.info()  </span><span style=\"color:#6A737D\"># Get a summary of the dataset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.describe()  </span><span style=\"color:#6A737D\"># Get a statistical summary of the dataset</span></span></code></pre>\n<p>In my dataset, I see that the mean (average) rating is 4.2 which means that most reviews are very positive. This is going to result in a model that is biased towards predicting 5 stars. In a later step, we will need to balance the dataset.</p>\n<p>This is an <em>example</em> of the kind of thing you might discover when exploring your dataset. <a href=\"https://www.urbandictionary.com/define.php?term=ymmv\" target=\"_blank\" rel=\"noopener noreferrer\">\nYMMV</a>!</p>\n<h2 id=\"step-4-clean-the-data\">Step 4: Clean the data</h2>\n<p>This step and the next take the most thinking. In this step, we want to wrangle the data into a format that we can use to train our model. The data should be free from:</p>\n<ul>\n<li>irrelevant data. Eliminate columns and parts of columns that we don’t need to train our model</li>\n<li>missing values</li>\n<li>nonsensical values (scores > 5 stars or &#x3C; 1, whole numbers only, etc.)</li>\n<li>duplicates</li>\n</ul>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> Your mission and dataset will vary, so this step will be different for each situation and unfortunately, you need to know your data wrangling skills to get this right.</p>\n</blockquote>\n<hr>\n<p>In this case, we want to predict the star rating based on the review text. So here are my steps:</p>\n<ol>\n<li>Drop any columns that are not relevant to our prediction. We only need the Score, Summary, and Text columns.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># or</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.drop([</span><span style=\"color:#9ECBFF\">'Id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ProductId'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'UserId'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ProfileName'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'HelpfulnessNumerator'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'HelpfulnessDenominator'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Time'</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">inplace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<ol start=\"2\">\n<li>Drop any rows that have missing values.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df.isnull().sum()  </span><span style=\"color:#6A737D\"># Check for missing values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.dropna(</span><span style=\"color:#FFAB70\">inplace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Drop them</span></span></code></pre>\n<p>This dataset had only 27 missing Summary values so we dropped them.</p>\n<ol start=\"3\">\n<li>Check for weird values. Score must be a whole number between 1 and 5.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">].unique()  </span><span style=\"color:#6A737D\"># Check unique values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df[(df[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (df[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">)]  </span><span style=\"color:#6A737D\"># Keep only valid scores</span></span></code></pre>\n<p>This example dataset had only good Scores. Nothing needed to be dropped but I kept the command in the cell above for your reference.</p>\n<ol start=\"4\">\n<li>Check for duplicates.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">df.duplicated().sum()  </span><span style=\"color:#6A737D\"># Check for duplicates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">df.drop_duplicates(</span><span style=\"color:#FFAB70\">inplace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Drop duplicates</span></span></code></pre>\n<p>This dataset had 173,000 duplicates. So we dropped them, leaving us about 395,000 rows.</p>\n<ol start=\"5\">\n<li>Redistribute the dataset.</li>\n</ol>\n<p>Remember from above that the dataset is biased towards 5 stars?\n<img src=\"https://res.cloudinary.com/rappayne/image/upload/v1747419464/kaggle_score_dist_udwbmh.png\" alt=\"There are more 5-star reviews than all other ratings combined\"></p>\n<p>If our intent were to predict an overall star rating keeping in mind the entire dataset, we would want to keep the balance and our predictions would be centered around 4.2 stars. But we want to predict the star rating <u>based on the review text</u> only, ignoring all other ratings. We need to balance the dataset to have an equal number of each star rating, 1 through 5. This is called a “stratified sample”.</p>\n<p>The rating with the fewest occurrences is 2 stars. There were 20,845 of them. So I decided to randomly sample 20,845 reviews from each of the Score groups.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> df.groupby(</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">).apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> x: x.sample(</span><span style=\"color:#79B8FF\">20_845</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">include_groups</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">).reset_index(</span><span style=\"color:#FFAB70\">drop</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">].value_counts()</span></span></code></pre>\n<p>Now each Score group has 20,845 reviews - an even distribution.</p>\n<h2 id=\"step-5-clean-the-text-data\">Step 5: Clean the text data</h2>\n<p>The human-entered Summary and Text is messy. Let’s clean them up with <a href=\"https://www.nltk.org/\" target=\"_blank\" rel=\"noopener noreferrer\">nltk</a>, a Python-based natural language toolkit.</p>\n<hr>\n<blockquote>\n<p><strong>Tip:</strong> You can also use <a href=\"https://spacy.io/\" target=\"_blank\" rel=\"noopener noreferrer\">spaCy</a> for this.</p>\n</blockquote>\n<hr>\n<ol>\n<li>Convert all text to lowercase so that “Best”, “best” and “BEST” are treated as the same word.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].str.lower()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].str.lower()</span></span></code></pre>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> At this point I was tempted to remove all the HTML tags from the text. But I decided to leave them in for this example. If you want to remove them, use <a href=\"https://www.geeksforgeeks.org/how-to-remove-html-tags-from-string-in-python/\" target=\"_blank\" rel=\"noopener noreferrer\">this</a> code.</p>\n</blockquote>\n<hr>\n<ol start=\"2\">\n<li>Remove Punctuation</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].str.translate(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">.maketrans(</span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, string.punctuation))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].str.translate(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">.maketrans(</span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, string.punctuation))</span></span></code></pre>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> Punctuation adds meaning but we need to keep it simple for this example. If you want to keep it in, use <a href=\"https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/\" target=\"_blank\" rel=\"noopener noreferrer\">\nBERT</a> or another transformer that can handle punctuation.</p>\n</blockquote>\n<hr>\n<ol start=\"3\">\n<li>Tokenize the sentences into individual words</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> nltk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> nltk.tokenize </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> word_tokenize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">nltk.download(</span><span style=\"color:#9ECBFF\">'punkt'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">nltk.download(</span><span style=\"color:#9ECBFF\">'punkt_tab'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].apply(word_tokenize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].apply(word_tokenize)</span></span></code></pre>\n<p>This converts the sentences into lists of words. For example, “Best product ever!” becomes [“best”, “product”, “ever”].</p>\n<ol start=\"4\">\n<li>Remove common words</li>\n</ol>\n<p>Words like “I”, “you”, “and”, “the”, “is”, “are”, etc. are called stop words. They don’t add much meaning to the text and can be removed. <code>nltk</code> provides us with a list but it includes negation words like “not” and “no”. That’s pretty bad in our case. Just imagine if you stripped “not” from this sentence: “It was not bad at all!”. It completely changes the meaning; it changes the <a href=\"https://statisticseasily.com/glossario/what-is-valence-understanding-its-role-in-data-science/\" target=\"_blank\" rel=\"noopener noreferrer\"><em>valence</em></a>. To stay truer to the origninal valence, we’re excluding the negation stop words from nltk’s list. You’ll likely want to do this for any NLP (natural language processing).</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> nltk.corpus </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> stopwords</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">negation_words </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'no'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'not'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'none'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'neither'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'never'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'nobody'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'nothing'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'nowhere'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'dont'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">nltk.download(</span><span style=\"color:#9ECBFF\">'stopwords'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stop_words </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(stopwords.words(</span><span style=\"color:#9ECBFF\">'english'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Allow negation stop words -- they're important for TF-IDF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">filtered_stop_words </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stop_words </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> negation_words</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: [word </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> filtered_stop_words])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: [word </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> filtered_stop_words])</span></span></code></pre>\n<ol start=\"5\">\n<li>Reduce words to their root form</li>\n</ol>\n<p><a href=\"https://www.techtarget.com/searchenterpriseai/definition/lemmatization\" target=\"_blank\" rel=\"noopener noreferrer\">Lemmatization</a> is reducing words to their base, like “running”→“run” or “better”→“good”.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> nltk.stem </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> WordNetLemmatizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">nltk.download(</span><span style=\"color:#9ECBFF\">'wordnet'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">lemmatizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> WordNetLemmatizer()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: [lemmatizer.lemmatize(word) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: [lemmatizer.lemmatize(word) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> word </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens])</span></span></code></pre>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> You can also use stemming instead of lemmatization. Stemming is faster but less accurate. Stemming removes prefixes and suffixes, like “happiness”→“happi” or “played”→“play”. Use nltk’s <a href=\"https://www.nltk.org/api/nltk.stem.porter.html#nltk.stem.porter.PorterStemmer\" target=\"_blank\" rel=\"noopener noreferrer\">PorterStemmer</a> or <a href=\"https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.SnowballStemmer\" target=\"_blank\" rel=\"noopener noreferrer\">SnowballStemmer</a> for this.</p>\n</blockquote>\n<hr>\n<ol start=\"6\">\n<li>Join the tokens back into a single string</li>\n</ol>\n<p>We broke them into “words” so each word could be processed. All the words are now clean, so we must join them back together into meaningful sentences for training.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: </span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">.join(tokens))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">].apply(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> tokens: </span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">.join(tokens))</span></span></code></pre>\n<p>We now have sentence again, but sentences that sound like <a href=\"https://www.youtube.com/watch?v=_K-L9uhsBLM\" target=\"_blank\" rel=\"noopener noreferrer\">Kevin from The Office</a>. Instead of “I love this extremely good product!”, we have “love extreme good product”.</p>\n<h2 id=\"step-6-split-the-data-into-training-and-testing-sets\">Step 6: Split the Data into Training and Testing Sets</h2>\n<p>To evaluate your model, you’ll split the data into training and testing sets:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.model_selection </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> train_test_split</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Define features (X) and target (y)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stratified_sample[</span><span style=\"color:#9ECBFF\">'Score'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Split the data (80% training, 20% testing)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">(X_train, X_test, y_train, y_test) </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> train_test_split(X, y, </span><span style=\"color:#FFAB70\">test_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">random_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<h2 id=\"step-7-vectorize-the-text-data\">Step 7: Vectorize the text data</h2>\n<p>Machine learning algorithms work with numerical data but our Summary and Text are strings. So we need to convert them into a numerical format. There are many algorithms to do this. We’ll use <a href=\"https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\" target=\"_blank\" rel=\"noopener noreferrer\">\nTF-IDF</a> from scikit-learn.</p>\n<ol>\n<li>Vectorize the text data.</li>\n</ol>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.feature_extraction.text </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TfidfVectorizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">vectorizer_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TfidfVectorizer(</span><span style=\"color:#FFAB70\">max_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10_000</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\"># Limit to the most important 10,000 unique words</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">vectorizer_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TfidfVectorizer(</span><span style=\"color:#FFAB70\">max_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10_000</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_summary.fit_transform(X_train[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_test_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_summary.transform(X_test[</span><span style=\"color:#9ECBFF\">'Summary'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_text.fit_transform(X_train[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_test_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_text.transform(X_test[</span><span style=\"color:#9ECBFF\">'Text'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Combine the two vectors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> scipy.sparse </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_train_combined </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sp.hstack([X_train_summary, X_train_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">X_test_combined </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sp.hstack([X_test_summary, X_test_text])</span></span></code></pre>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> I was tempted to fit the entire dataset, then split the training and testing sets and transform them individually. But that would have leaked information from the test set into the training set. When testing is done later, the accuracy score would have been exaggerated. Think about it; when running the model in the future, we have no idea what words will be used. For an accurate test, we must treat the test set as if it were new data.</p>\n</blockquote>\n<blockquote>\n<p>Look <a href=\"https://stackoverflow.com/a/43296172/88373\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> for a great explanation of fit() vs transform() vs fit_transform().</p>\n</blockquote>\n<hr>\n<h2 id=\"step-8-train-a-machine-learning-model\">Step 8: Train a Machine Learning Model</h2>\n<p>Choose a machine learning algorithm and train your model. Since all of our outcomes are in categories (1 through 5), we’re going to choose a Random Forest classifier.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.ensemble </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RandomForestClassifier</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RandomForestClassifier(</span><span style=\"color:#FFAB70\">random_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(X_train_combined, y_train)  </span><span style=\"color:#6A737D\"># &#x3C;-- Train the model</span></span></code></pre>\n<hr>\n<blockquote>\n<p><strong>Note:</strong> If it were a regression problem (predicting a continuous value), you would use a different algorithm, like <a href=\"https://www.geeksforgeeks.org/ml-linear-regression/\" target=\"_blank\" rel=\"noopener noreferrer\">Linear Regression</a> or <a href=\"https://www.geeksforgeeks.org/decision-tree-introduction-example/\" target=\"_blank\" rel=\"noopener noreferrer\">Decision Trees</a>.</p>\n</blockquote>\n<hr>\n<h2 id=\"step-9-evaluate-the-model\">Step 9: Evaluate the Model</h2>\n<p>We’ve now got a model that can predict the star rating based on the review text. Let’s see how well it works by evaluating it on the test set.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.metrics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> accuracy_score</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Make predictions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y_pred </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(X_test_combined)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">accuracy </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> accuracy_score(y_test, y_pred)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Model Accuracy: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">accuracy</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<p>The accuracy score will be between 0 and 1. A score of 0.8 means that the model correctly predicted the star rating 80% of the time. Ours was a dismal 0.51. 😦 This may have been because a close guess is still seen as 100% wrong (unlike in a regression model). So if we predicted a 2-star review but it was 1 or if we predicted a 4-star review and it was 5, those would still be considered completely wrong. In classification, there’s no such thing as close. Let’s try with some hand-written reviews.</p>\n<h2 id=\"step-10-perform-a-sanity-check\">Step 10: Perform a Sanity Check</h2>\n<p>Perform a quick sanity check by predicting a few single values. We’ll write three reviews, one each for a 1-star, 3-star, and 5-star rating. Let’s how closely the model predicts.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">test_5_star_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'Wonderful!'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_5_star_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'This is an amazing product. I love it!'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_5_star_vectorized_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_summary.transform([test_5_star_summary])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_5_star_vectorized_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_text.transform([test_5_star_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_5_star </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sp.hstack([test_5_star_vectorized_summary, test_5_star_vectorized_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">prediction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(test_5_star)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected 5 star. Actual prediction: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prediction</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_1_star_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'Terrible'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_1_star_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'This was a very bad product. I hated it!'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_1_star_vectorized_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_summary.transform([test_1_star_summary])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_1_star_vectorized_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_text.transform([test_1_star_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_1_star </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sp.hstack([test_1_star_vectorized_summary, test_1_star_vectorized_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">prediction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(test_1_star)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected 1 star. Actual prediction: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prediction</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_3_star_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'Just okay'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_3_star_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'It was alright I guess. Barely worked.'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_3_star_vectorized_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_summary.transform([test_3_star_summary])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_3_star_vectorized_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> vectorizer_text.transform([test_3_star_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_3_star </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sp.hstack([test_3_star_vectorized_summary, test_3_star_vectorized_text])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">prediction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(test_3_star)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected 3 stars. Actual prediction: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prediction</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<p>These resulted in predictions of 5, 1, and 3 stars respectively. Perfect! Looks like our model works pretty well after all.</p>\n<h2 id=\"step-11-save-the-model-for-future-use\">Step 11: Save the Model for Future Use</h2>\n<p>Finally, let’s save our trained model in Python’s <code>pickle</code> format so we can use it later without retraining. This is especially useful if you’re going to deploy the model in a web app or use it in another project.</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">filename </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'star_rating_from_Summary_and_Text.pkl'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(filename, </span><span style=\"color:#9ECBFF\">'wb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  pickle.dump(model, f)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Model saved as </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">filename</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre>\n<p>You can now download the <code>trained_model.pkl</code> file from Colab and use it in other projects.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>If you made it this far, nice work! You’ve successfully trained a machine learning model in Google Colab using a Kaggle dataset. This workflow — dataset selection, data wrangling, training, evaluation, and saving — is pretty much the same for any ML training project. Of course, the methods will vary wildly depending on the dataset and the problem you’re trying to solve. But the overall process is the same.</p>\n<p><a href=\"https://agilegadgets.com/about\">Reach out</a> if you have questions or comments or want some training. I’m always happy to help.</p>";

				const frontmatter = {"layout":"../../layouts/BlogPost.astro","title":"Using a Kaggle Dataset to Train a ML Model in Google Colab","description":"Learn how to train a machine learning model in Google Colab using a dataset from Kaggle. This step-by-step tutorial will guide you through dataset selection, training, evaluation, and saving your model for future use.","pubDate":"2025-05-28T00:00:00.000Z","author":"Rap Payne","time-to-read":"15 minutes","url":"/blog/kaggle-dataset-google-colab","cloudinaryImageFileName":"v1747756929/kaggle_colab_training_vzva5i.png","categories":["Google Colab","Kaggle","Machine Learning","Python","Tutorial"]};
				const file = "/Users/rap/Desktop/agile-gadgets/src/pages/blog/kaggle-dataset-google-colab.md";
				const url = "/blog/kaggle-dataset-google-colab";
				function rawContent() {
					return "   \n                                    \n                                                                   \n                                                                                                                                                                                                                                        \n                   \n                 \n                        \n                                      \n                                                                     \n                                                                                \n   \n\nIn a [prior post](https://agilegadgets.com/blog/react-hugging-face-inference-api), we learned how to write a React app that uses a Hugging Face ML model to predict data. But what if there's no ML model that fits your needs? Well, if you have a bunch of data, you can train your own model. Over the next few minutes, we'll show you how to find a dataset on Kaggle, train a model in Google Colab, and save it for future use. \n\nLet's say that you want to be able to predict the star rating of a product based on the comment that the user is leaving. If they say \"Best product ever!\", they'll probably give it 5 stars. If they say \"This product sucks!\", they'll probably give it 1 star. \n\nWe will want to:\n1. [Find a dataset on Kaggle.](#step-1-find-a-dataset-on-kaggle)\n1. [Create a Google Colab notebook to train our model.](#step-2-set-up-google-colab)\n1. [Get the dataset into Google Colab.](#step-3-get-the-kaggle-dataset-into-google-colab)\n1. [Wrangle the data to get it ready for training.](#step-4-clean-the-data)\n1. [Split the data into training and testing sets.](#step-6-split-the-data-into-training-and-testing-sets)\n1. [Convert the text data into a numerical format.](#step-7-vectorize-the-text-data)\n1. [Train a model to predict the star rating based on the review.](#step-8-train-a-machine-learning-model)\n1. [Evaluate the model to see how well it works.](#step-9-evaluate-the-model)\n1. [Perform a sanity check to see if the model works.](#step-10-perform-a-sanity-check)\n1. [Save the trained model so we can use it in an app.](#step-11-save-the-model-for-future-use)\n\n## Step 1: Find a Dataset on Kaggle\n\n[Kaggle](https://www.kaggle.com/) has a ton of datasets and they're organized well.\n\n1. Go to [Kaggle Datasets](https://www.kaggle.com/datasets).\nYou can either browse the datasets or use the search box to find a dataset that fits your needs. I'm going to search for \"product reviews\". The first result I see is called <a href=\"https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Product Reviews</a>. \n1. Once you've found a dataset, download it to your local machine and unzip it. \n\n---\n> **Tip:** [There are ways](https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab) to get the dataset into Colab without downloading it to your local machine, but for this tutorial, we'll keep it simple and download it locally.\n---\n\n## Step 2: Set Up Google Colab\n\nGoogle Colab is a free cloud-based Jupyter notebook environment.\n\n1. Go to [Google Colab](https://colab.research.google.com/).\n2. Create a **+New Notebook**.\n1. You might want to rename the notebook to something meaningful. I'm going with \"Kaggle Product Reviews\".\n1. Upload your Kaggle dataset to the notebook by clicking on the folder icon in the left sidebar and hitting the \"Upload to session storage\" button.\n---\n> **Note:** The uploaded file will only be available during the current session. If you close the notebook, you'll need to re-upload the file.\n---\n\n## Step 3: Get the Kaggle dataset into Google Colab\n1. Import the file.\n```python\nimport pandas as pd\ntry:\n  df = pd.read_csv('Reviews.csv')\n  print(df.head())\nexcept FileNotFoundError:\n  print(\"Error: Reviews.csv not found. Make sure the file exists.\")\nexcept pd.errors.ParserError:\n  print(\"Error: Couldn't parse Reviews.csv. Please check the file format.\")\nexcept Exception as e:\n  print(f\"An unexpected error occurred: {e}\")\n```\n2. Understand the data.\n```python\ndf.head()  # Display the first few rows of the dataset\ndf.shape  # Get the shape of the dataset (rows, columns)\ndf.info()  # Get a summary of the dataset\ndf.describe()  # Get a statistical summary of the dataset\n```\nIn my dataset, I see that the mean (average) rating is 4.2 which means that most reviews are very positive. This is going to result in a model that is biased towards predicting 5 stars. In a later step, we will need to balance the dataset. \n\nThis is an _example_ of the kind of thing you might discover when exploring your dataset. <a href=\"https://www.urbandictionary.com/define.php?term=ymmv\" target=\"_blank\" rel=\"noopener noreferrer\">\nYMMV</a>!\n\n## Step 4: Clean the data\nThis step and the next take the most thinking. In this step, we want to wrangle the data into a format that we can use to train our model. The data should be free from:\n- irrelevant data. Eliminate columns and parts of columns that we don't need to train our model\n- missing values\n- nonsensical values (scores > 5 stars or < 1, whole numbers only, etc.)\n- duplicates\n\n---\n> **Note:** Your mission and dataset will vary, so this step will be different for each situation and unfortunately, you need to know your data wrangling skills to get this right. \n---\n\nIn this case, we want to predict the star rating based on the review text. So here are my steps:\n\n 1. Drop any columns that are not relevant to our prediction. We only need the Score, Summary, and Text columns.\n```python\ndf = df[['Score', 'Summary', 'Text']]\n# or\ndf.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time'], axis=1, inplace=True)\n```\n\n2. Drop any rows that have missing values.\n```python\ndf.isnull().sum()  # Check for missing values\ndf.dropna(inplace=True)  # Drop them\n```\nThis dataset had only 27 missing Summary values so we dropped them.\n\n3. Check for weird values. Score must be a whole number between 1 and 5.\n```python\ndf['Score'].unique()  # Check unique values\ndf = df[(df['Score'] >= 1) & (df['Score'] <= 5)]  # Keep only valid scores\n```\nThis example dataset had only good Scores. Nothing needed to be dropped but I kept the command in the cell above for your reference.\n\n4. Check for duplicates.\n```python\ndf.duplicated().sum()  # Check for duplicates\ndf.drop_duplicates(inplace=True)  # Drop duplicates\n```\nThis dataset had 173,000 duplicates. So we dropped them, leaving us about 395,000 rows.\n\n5. Redistribute the dataset. \n\nRemember from above that the dataset is biased towards 5 stars? \n![There are more 5-star reviews than all other ratings combined](https://res.cloudinary.com/rappayne/image/upload/v1747419464/kaggle_score_dist_udwbmh.png)\n\nIf our intent were to predict an overall star rating keeping in mind the entire dataset, we would want to keep the balance and our predictions would be centered around 4.2 stars. But we want to predict the star rating <u>based on the review text</u> only, ignoring all other ratings. We need to balance the dataset to have an equal number of each star rating, 1 through 5. This is called a \"stratified sample\". \n\nThe rating with the fewest occurrences is 2 stars. There were 20,845 of them. So I decided to randomly sample 20,845 reviews from each of the Score groups.\n\n```python\nstratified_sample = df.groupby('Score').apply(lambda x: x.sample(20_845), include_groups=True).reset_index(drop=True)\nstratified_sample['Score'].value_counts()\n```\nNow each Score group has 20,845 reviews - an even distribution.\n\n## Step 5: Clean the text data\n\nThe human-entered Summary and Text is messy. Let's clean them up with <a href=\"https://www.nltk.org/\" target=\"_blank\" rel=\"noopener noreferrer\">nltk</a>, a Python-based natural language toolkit.\n\n---\n> **Tip:** You can also use <a href=\"https://spacy.io/\" target=\"_blank\" rel=\"noopener noreferrer\">spaCy</a> for this.\n---\n\n1. Convert all text to lowercase so that \"Best\", \"best\" and \"BEST\" are treated as the same word.\n\n```python\nstratified_sample['Summary'] = stratified_sample['Summary'].str.lower()\nstratified_sample['Text'] = stratified_sample['Text'].str.lower()\n```\n---\n> **Note:** At this point I was tempted to remove all the HTML tags from the text. But I decided to leave them in for this example. If you want to remove them, use <a href=\"https://www.geeksforgeeks.org/how-to-remove-html-tags-from-string-in-python/\" target=\"_blank\" rel=\"noopener noreferrer\">this</a> code.\n---\n\n2. Remove Punctuation\n\n```python\nimport string\nstratified_sample['Summary'] = stratified_sample['Summary'].str.translate(str.maketrans('', '', string.punctuation))\nstratified_sample['Text'] = stratified_sample['Text'].str.translate(str.maketrans('', '', string.punctuation))\n```\n---\n> **Note:** Punctuation adds meaning but we need to keep it simple for this example. If you want to keep it in, use <a href=\"https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/\" target=\"_blank\" rel=\"noopener noreferrer\">\nBERT</a> or another transformer that can handle punctuation.\n---\n\n3. Tokenize the sentences into individual words\n\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nstratified_sample['Summary'] = stratified_sample['Summary'].apply(word_tokenize)\nstratified_sample['Text'] = stratified_sample['Text'].apply(word_tokenize)\n```\nThis converts the sentences into lists of words. For example, \"Best product ever!\" becomes [\"best\", \"product\", \"ever\"].\n\n4. Remove common words \n\nWords like \"I\", \"you\", \"and\", \"the\", \"is\", \"are\", etc. are called stop words. They don't add much meaning to the text and can be removed. `nltk` provides us with a list but it includes negation words like \"not\" and \"no\". That's pretty bad in our case. Just imagine if you stripped \"not\" from this sentence: \"It was not bad at all!\". It completely changes the meaning; it changes the <a href=\"https://statisticseasily.com/glossario/what-is-valence-understanding-its-role-in-data-science/\" target=\"_blank\" rel=\"noopener noreferrer\">*valence*</a>. To stay truer to the origninal valence, we're excluding the negation stop words from nltk's list. You'll likely want to do this for any NLP (natural language processing).\n\n```python\nfrom nltk.corpus import stopwords\n\nnegation_words = {\n    'no',\n    'not',\n    'none',\n    'neither',\n    'never',\n    'nobody',\n    'nothing',\n    'nowhere',\n    'dont'\n}\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Allow negation stop words -- they're important for TF-IDF\nfiltered_stop_words = stop_words - negation_words\n\nstratified_sample['Summary'] = stratified_sample['Summary'].apply(lambda tokens: [word for word in tokens if word not in filtered_stop_words])\nstratified_sample['Text'] = stratified_sample['Text'].apply(lambda tokens: [word for word in tokens if word not in filtered_stop_words])\n```\n\n5. Reduce words to their root form\n\n<a href=\"https://www.techtarget.com/searchenterpriseai/definition/lemmatization\" target=\"_blank\" rel=\"noopener noreferrer\">Lemmatization</a> is reducing words to their base, like \"running\"→\"run\" or \"better\"→\"good\".\n```python\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\nstratified_sample['Summary'] = stratified_sample['Summary'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\nstratified_sample['Text'] = stratified_sample['Text'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n```\n---\n> **Note:** You can also use stemming instead of lemmatization. Stemming is faster but less accurate. Stemming removes prefixes and suffixes, like \"happiness\"→\"happi\" or \"played\"→\"play\". Use nltk's <a href=\"https://www.nltk.org/api/nltk.stem.porter.html#nltk.stem.porter.PorterStemmer\" target=\"_blank\" rel=\"noopener noreferrer\">PorterStemmer</a> or <a href=\"https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.SnowballStemmer\" target=\"_blank\" rel=\"noopener noreferrer\">SnowballStemmer</a> for this.\n---\n\n6. Join the tokens back into a single string\n\nWe broke them into \"words\" so each word could be processed. All the words are now clean, so we must join them back together into meaningful sentences for training.\n```python\nstratified_sample['Summary'] = stratified_sample['Summary'].apply(lambda tokens: ' '.join(tokens))\nstratified_sample['Text'] = stratified_sample['Text'].apply(lambda tokens: ' '.join(tokens))\n```\nWe now have sentence again, but sentences that sound like <a href=\"https://www.youtube.com/watch?v=_K-L9uhsBLM\" target=\"_blank\" rel=\"noopener noreferrer\">Kevin from The Office</a>. Instead of \"I love this extremely good product!\", we have \"love extreme good product\". \n\n## Step 6: Split the Data into Training and Testing Sets\n\nTo evaluate your model, you'll split the data into training and testing sets:\n\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Define features (X) and target (y)\nX = stratified_sample[['Summary','Text']]\ny = stratified_sample['Score']\n\n# Split the data (80% training, 20% testing)\n(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n## Step 7: Vectorize the text data\nMachine learning algorithms work with numerical data but our Summary and Text are strings. So we need to convert them into a numerical format. There are many algorithms to do this. We'll use <a href=\"https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\" target=\"_blank\" rel=\"noopener noreferrer\">\nTF-IDF</a> from scikit-learn. \n\n1. Vectorize the text data.\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_summary = TfidfVectorizer(max_features=10_000) # Limit to the most important 10,000 unique words\nvectorizer_text = TfidfVectorizer(max_features=10_000) \n\nX_train_summary = vectorizer_summary.fit_transform(X_train['Summary'])\nX_test_summary = vectorizer_summary.transform(X_test['Summary'])\nX_train_text = vectorizer_text.fit_transform(X_train['Text'])\nX_test_text = vectorizer_text.transform(X_test['Text'])\n\n# Combine the two vectors\nimport scipy.sparse as sp\nX_train_combined = sp.hstack([X_train_summary, X_train_text])\nX_test_combined = sp.hstack([X_test_summary, X_test_text])\n```\n---\n> **Note:** I was tempted to fit the entire dataset, then split the training and testing sets and transform them individually. But that would have leaked information from the test set into the training set. When testing is done later, the accuracy score would have been exaggerated. Think about it; when running the model in the future, we have no idea what words will be used. For an accurate test, we must treat the test set as if it were new data.\n\n> Look <a href=\"https://stackoverflow.com/a/43296172/88373\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> for a great explanation of fit() vs transform() vs fit_transform().\n---\n\n## Step 8: Train a Machine Learning Model\n\nChoose a machine learning algorithm and train your model. Since all of our outcomes are in categories (1 through 5), we're going to choose a Random Forest classifier.\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_combined, y_train)  # <-- Train the model\n```\n---\n> **Note:** If it were a regression problem (predicting a continuous value), you would use a different algorithm, like <a href=\"https://www.geeksforgeeks.org/ml-linear-regression/\" target=\"_blank\" rel=\"noopener noreferrer\">Linear Regression</a> or <a href=\"https://www.geeksforgeeks.org/decision-tree-introduction-example/\" target=\"_blank\" rel=\"noopener noreferrer\">Decision Trees</a>.\n---\n\n## Step 9: Evaluate the Model\nWe've now got a model that can predict the star rating based on the review text. Let's see how well it works by evaluating it on the test set. \n\n```python\nfrom sklearn.metrics import accuracy_score\n\n# Make predictions\ny_pred = model.predict(X_test_combined)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.2f}\")\n```\nThe accuracy score will be between 0 and 1. A score of 0.8 means that the model correctly predicted the star rating 80% of the time. Ours was a dismal 0.51. 😦 This may have been because a close guess is still seen as 100% wrong (unlike in a regression model). So if we predicted a 2-star review but it was 1 or if we predicted a 4-star review and it was 5, those would still be considered completely wrong. In classification, there's no such thing as close. Let's try with some hand-written reviews.\n\n## Step 10: Perform a Sanity Check\n\nPerform a quick sanity check by predicting a few single values. We'll write three reviews, one each for a 1-star, 3-star, and 5-star rating. Let's how closely the model predicts.\n\n```python\ntest_5_star_summary = 'Wonderful!' \ntest_5_star_text = 'This is an amazing product. I love it!' \ntest_5_star_vectorized_summary = vectorizer_summary.transform([test_5_star_summary])\ntest_5_star_vectorized_text = vectorizer_text.transform([test_5_star_text])\ntest_5_star = sp.hstack([test_5_star_vectorized_summary, test_5_star_vectorized_text])\nprediction = model.predict(test_5_star)\nprint(f\"Expected 5 star. Actual prediction: {prediction}\")\n\ntest_1_star_summary = 'Terrible' \ntest_1_star_text = 'This was a very bad product. I hated it!' \ntest_1_star_vectorized_summary = vectorizer_summary.transform([test_1_star_summary])\ntest_1_star_vectorized_text = vectorizer_text.transform([test_1_star_text])\ntest_1_star = sp.hstack([test_1_star_vectorized_summary, test_1_star_vectorized_text])\nprediction = model.predict(test_1_star)\nprint(f\"Expected 1 star. Actual prediction: {prediction}\")\n\ntest_3_star_summary = 'Just okay' \ntest_3_star_text = 'It was alright I guess. Barely worked.' \ntest_3_star_vectorized_summary = vectorizer_summary.transform([test_3_star_summary])\ntest_3_star_vectorized_text = vectorizer_text.transform([test_3_star_text])\ntest_3_star = sp.hstack([test_3_star_vectorized_summary, test_3_star_vectorized_text])\nprediction = model.predict(test_3_star)\nprint(f\"Expected 3 stars. Actual prediction: {prediction}\")\n```\nThese resulted in predictions of 5, 1, and 3 stars respectively. Perfect! Looks like our model works pretty well after all.\n\n## Step 11: Save the Model for Future Use\n\nFinally, let's save our trained model in Python's `pickle` format so we can use it later without retraining. This is especially useful if you're going to deploy the model in a web app or use it in another project.\n\n```python\nimport pickle\nfilename = 'star_rating_from_Summary_and_Text.pkl'\nwith open(filename, 'wb') as f:\n  pickle.dump(model, f)\nprint(f\"Model saved as {filename}\")\n```\n\nYou can now download the `trained_model.pkl` file from Colab and use it in other projects.\n\n## Conclusion\n\nIf you made it this far, nice work! You've successfully trained a machine learning model in Google Colab using a Kaggle dataset. This workflow — dataset selection, data wrangling, training, evaluation, and saving — is pretty much the same for any ML training project. Of course, the methods will vary wildly depending on the dataset and the problem you're trying to solve. But the overall process is the same.\n\n[Reach out](https://agilegadgets.com/about) if you have questions or comments or want some training. I'm always happy to help.";
				}
				async function compiledContent() {
					return await html();
				}
				function getHeadings() {
					return [{"depth":2,"slug":"step-1-find-a-dataset-on-kaggle","text":"Step 1: Find a Dataset on Kaggle"},{"depth":2,"slug":"step-2-set-up-google-colab","text":"Step 2: Set Up Google Colab"},{"depth":2,"slug":"step-3-get-the-kaggle-dataset-into-google-colab","text":"Step 3: Get the Kaggle dataset into Google Colab"},{"depth":2,"slug":"step-4-clean-the-data","text":"Step 4: Clean the data"},{"depth":2,"slug":"step-5-clean-the-text-data","text":"Step 5: Clean the text data"},{"depth":2,"slug":"step-6-split-the-data-into-training-and-testing-sets","text":"Step 6: Split the Data into Training and Testing Sets"},{"depth":2,"slug":"step-7-vectorize-the-text-data","text":"Step 7: Vectorize the text data"},{"depth":2,"slug":"step-8-train-a-machine-learning-model","text":"Step 8: Train a Machine Learning Model"},{"depth":2,"slug":"step-9-evaluate-the-model","text":"Step 9: Evaluate the Model"},{"depth":2,"slug":"step-10-perform-a-sanity-check","text":"Step 10: Perform a Sanity Check"},{"depth":2,"slug":"step-11-save-the-model-for-future-use","text":"Step 11: Save the Model for Future Use"},{"depth":2,"slug":"conclusion","text":"Conclusion"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${renderComponent(result, 'Layout', $$BlogPost, {
								file,
								url,
								content,
								frontmatter: content,
								headings: getHeadings(),
								rawContent,
								compiledContent,
								'server:root': true,
							}, {
								'default': () => renderTemplate`${unescapeHTML(html())}`
							})}`;
				});

const _page = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
	__proto__: null,
	Content,
	compiledContent,
	default: Content,
	file,
	frontmatter,
	getHeadings,
	rawContent,
	url
}, Symbol.toStringTag, { value: 'Module' }));

export { _page as _ };
